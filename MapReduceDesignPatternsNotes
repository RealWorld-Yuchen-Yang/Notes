Design Patterns and MapReduce
1. The map tasks generally,
   load, parse, transform and filter data.
   The reduce tasks are responsible for
   handling a subset of the map task output.

2. map job phases
  2.1 record reader
        it passes the data to the mapper in the form of a key/value pair.
        usually the key in this context is positional information,
        and the value is the chunk of data that composes a record.
  2.2 mapper
        user-provided code is executed on each key/value pair from the record reader,
        so as to produce the intermediate pairs.
        Note:
           the semantics of the user-provided code varies on the purpose of the mapper job
  2.3 combiner
        an optional localized reducer,
        group data in the map phase,
        which can reduce data to be passed via the internet
  2.4 partitioner
        takes the intermediate key/value pairs from the mapper/combiner,
        and split them up into shards,
        one shard per reducer.
        usually key/value pairs with the same key is partitioned together.
        it is achieved by: key.hashCode()%(#reducers)
        the partitioned data is written to the local file system for each map task and waits to be pulled by its respective reducer.
  2.5 output: intermediate keys and values, which are sent to the reducer

3. reduce job phases
  3.1 shuffle and sort
        this step takes the output files written by all of the partitioners and downloads them to the local machine of the reducers
        these individual data pieces are then sorted by key into one larger data list
        this step is taken care of by the framework
  3.2 reducer
        takes the grouped data as input
        run a reduce function once per key grouping
  3.3 output format
        translates the final key/value pair from the reduce function and writes it out to a file by a record writer.
        data is written out to HDFS.
        Note:
          anything we pass to context.write will get written out to a file.
          each reducer will create one file,
          so if you want to coalesce them together,
          you will have to write a post-processing step to concatenate them.

Summarization Patterns
1. Numerical summarizations
  1.1 applicability:
    1.1.1 when you are dealing with numerical data or counting
    1.1.2 when data can be grouped by specific fields
  1.2 known uses
    1.2.1 word count
    1.2.2 record count
    1.2.3 min/max/count
    1.2.4 average/median/standard/deviation

2. inverted index
  2.1 applicability:
        inverted indexes should be used when quick search query responses are required.
        The results of such a query can be preprocessed and ingested into a database
  2.2 consequences:
        the final output is a set of part files that
        contain a mapping of field value to a set of unique IDs of records containing the associated field value
        that is <fieldContent, Collection<ID>>
  2.3 example:
        Wikipedia reference inverted index

3. counting with counters
  3.1 description
        this pattern utilizes the MapReduce framework's counters utility
        to calculate a global sum entirely on the map side without producing any output
  3.2 structure
        the mapper processes each input record at a time to increment counters based on certain criteria.
        the counters are then aggregated by the TaskTrackers running the tasks
        and incrementally reported to the JobTracker for overall aggregation upon job success.
        Note:
          this job is map only,
          there is no combiner, partitioner, or reducer required.

Filtering Patterns
1. Filtering
2. Bloom filtering
3. Top Ten
4. Distinct
