Introduction
1. Concerns of "utilization", "fairness", and "convenience" motivates the development of
    process and threads.
    Threads allow multiple streams of program control flow to coexist within a process

2. Processes and Threads:
  2.1 Processes,
      are synonym to Applications or programs
  2.2 Threads,
      are sometimes called lightweight processes,
      and most modern operating systems treat threads, not processes as the basic units of scheduling

3. Thread is the basic unit of scheduling

4. Safety, Liveness, Performance
  4.1 Safety, means nothing bad ever happens
  4.2 Liveness, means something good eventually happens
  4.3 Performance, means something good enough happens

Thread Safety
1. object's state:
    its data, stored in state variables such as instance or static fields.

2. Shared,
    a variable could be accessed by multiple threads.

3. mutable,
    the field's value could change during its lifetime

4. making an object thread-safe requires using synchronization to coordinate access to its mutable state

5. The primary mechanism for synchronization in Java is the **synchronized** keyword
   which provides exclusive lock
   but the term "synchronization" also includes the use of **volatile variables**, **explicit lock**, **atomic variables**

6. If multiple threads access the same mutable state variable without appropriate synchronization,
   your program is broken.
   There are three ways to fix it
  6.1 do not share the state variable across threads
  6.2 make the state variable immutable, immutable objects are automatically thread-safe
  6.3 use synchronization whenever accessing the state variable

7. In any case, the concept of a thread-safe class makes sense only if the class encapsulates its own state.
   Thread safety may be a term that is applied to code,
   but it is about state, and it can only be applied to the entire body of code that encapsulates its state,
   which may be an object or an entire program

8. Definition of Thread-Safety
   A class is thread-safe if it behaves correctly when accessed from multiple threads,
   regardless of the scheduling or interleaving of the execution of those threads by the runtime environment,
   and with no additional synchronization or other coordination on the part of the calling code

9. Stateless objects are always thread-safe
   Stateless objects means:
   9.1 it has no fields and references no fields from other classes.
   9.2 the transient state for a particular computation exists solely in local variables that are stored on the thread's stack
       and are accessible only to the executing thread.

10. Atomicity
    atomic operations are either by nature (one command in JVM)
    or synthetic (in a synchronized block)

11. Race Condition:
    The possibility of incorrect results in the presence of unlucky timing
    A race condition occurs when the correctness of a computation depends on the relative timing or interleaving of multiple threads by the runtime.
    The most common type of race condition is check-then-act, read-modify-write
    where a potentially state observation is used to make a decision on what to do next.
    Solution is to introduce a common lock to make the "compound actions" atomic

12. data race:
    arises when synchronization is not used to coordinate all access to a shared non-final field.
    different operations may overwrite the previous operation's result

13. for shared components,
    even the shared field is private to the object itself,
    it still risk the thread-safety problem,
    only "method level variables" can escape from such issue

14. Sequences of operations that must be executed atomically in order to remain thread-safe
    and atomic is defined as:
      Operation A and B are atomic with respect to each other if,
      from the perspective of a thread executing A,
      when another thread executes B,
      either all of B has executed or none of it has.
      An atomic operation is one that is atomic with respect to all operations,
      including itself,
      that operate on the same state

15. **java.util.concurrent.atomic** package contains atomic variable classes
    which are used for effecting atomic state transitions on numbers and object references.

16. Servlet is stateless by nature,
    if we introduce states (fields) into a servlet,
    only if the introduced field is thread safe,
    will let the Servlet containing it remain thread-safe

17. When multiple variables participate in an invariant,
    they are not independent (the value of one constrains the allowed value(s) of the others),
    Thus when updating one,
    you must update the others in the same atomic operations
    In this case,
    even though each call to set is atomic,
    there is still a window of vulnerability when one field has been modified and the other has not,
    and during that time other threads could see that the invariant does not hold.
    Solution: update related state variables in a single atomic operation by using "Locking"

18. Java Synchronized Block, contains two parts
  18.1 a reference to an object that will server as the lock
  18.2 a block of code to be guarded by that lock

19. a synchronized method
  a shorthand for a synchronized block,
  span an entire method body,
  whose lock is the object on which the method is being invoked.
  Note: static synchronized methods use the Class object for the lock

20. while synchronized methods can make individual operations atomic,
    additional locking is required - when multiple operations are combined into a compound action.

21. to improve efficiency while remain thread-safe,
    we should make the synchronized block as small as possible,
    at the same time,
    the atomicity should be guaranteed.

Sharing Objects
1. Synchronization also has another significant and subtle aspect: memory visibility
   We should ensure that when a thread modifies the state of an object,
   other threads can actually see the changes that were made

2. Out-of-thin-air safety:
    value a thread sees is a random value,
    not even at least the thread a inconsistent/stale value that is actually put by other thread

3. 64-bit numeric variables (double and long) are not declared volatile

4. Intrinsic locking can be used to guarantee the one thread sees the effects of another in a predicable manner

5. Volatile variables:
    a weaker form of synchronization,
    it ensures that updates to a variable are propagated predictably to other threads.
    When a field is declared **volatile**,
    the compiler and runtime are put on notice that this variable is shared and that operations on it should not be rendered with other memory operations
    Volatile variables are not cached in registers or in caches where they are hidden from other processors.
    so a read of a volatile variable always returns the most recent write by any thread

6. accessing a volatile variable performs no locking
   and so cannot cause the executing thread to block
   making volatile variables a lighter weight synchronization mechanism than **synchronized**

7. It is not recommended relying too heavily on volatile variables for visibility.
   code that relies on volatile variables for visibility of arbitrary state is more fragile and harder to understand than code that uses locking

8. The most common use for volatile variables is as a [completion| interruption| status] flag
   You can use volatile variables only when all the following criteria are met:
   8.1 writes to the variable do not depend on its current value,
       or you can ensure that only a single thread ever update the value
   8.2 The variable does not participate in invariants with other state variables
   8.3 Locking is not required for any other reason while the variable is being accessed

9. Publication and Escape
  publishing an object means making it available to code outside of its current scope
  An object that is published when it should not have been is said to have escaped

10. Internal state can be published to its inner class, which is a case of Escape (bad idiom)

11. Safe Construction practices:
      an object is in a predictable, consistent state only after its constructor returns.
      so publishing an object from within its constructor can publish an incompletely constructed object.

12. A common mistake that can let the **this** reference escape during construction is to start a thread from a constructor

13. There is nothing wrong with creating a thread in a constructor,
    but it is best not to start the thread immediately

14. Thread confinement:
      data is only accessed from a single thread, no synchronization is needed.

15. Stack confinement:
      a special case of thread confinement,
      in stack confinement, an object can only be reached through local variables

16. ThreadLocal
      a more formal means of maintaining thread confinement is ThreadLocal.
      which allows you to associate a per-thread value with a value-holding object
      ThreadLocal provides get and set accessors that maintain a separate copy of the value for each thread that uses it.

17. Immutability
      An object whose fields are **all final** may still be mutable,
      since final fields can hold references to mutable objects
      An object is immutable if:
      17.1 Its state cannot be modified after construction
      17.2 All its fields are final
      17.3 It is properly constructed (the **this** reference does not escape during construction)

18. It is the use of final fields that makes possible the guarantee of initialization safety
    that lets immutable objects be freely accessed and shared without synchronization

19. Just as it is a good practice (encapsulation) to make all fields **private**
    It is a good practice to make all fields final unless they need to be mutable

20. Safe publication idioms
      To publish an object safely,
      both the reference to the object and the object's state must be made visible to other threads at the same time.
      A properly constructed object can be safely published by one of the following methods:
      20.1 Initializing an object reference from a static initializer.
      20.2 Storing a reference to it into a **volatile** field or **AtomicReference**
      20.3 Storing a reference to it into a final field of a properly constructed object
      20.4 Storing a reference to it into a field that is properly guarded by a lock

21. Effectively immutable (a weaker form of immutable)
      Objects that are not technically immutable,
      but whole state will not be modified after publication,
      are called effectively immutable

22. If an object may be modified after construction,
    safe publication ensures only the visibility of the **as-published state**
    Synchronization must be used **not only to publish** a mutable object
    **but also every time the object is accessed**, to ensure visibility of subsequent modifications

23. A thread-safe object performs synchronization internally

Composing Objects
1. If the object has fields that are references to other objects,
   its state will encompass fields from the referenced objects as well

2. Operations with state-based preconditions are called state-dependent

3. Java Monitor Pattern
    An object following the Java monitor pattern encapsulates all its mutable state
    and guards it with the object's own intrinsic lock.

4. Thread-Safety Delegation
  4.1 Definition:
        The thread-safety of the current class depends on the thread-safety of its fields
  4.2 Delegations successes,
        when the underlying fields are thread-safe and independent
  4.3 Delegations fails,
    4.3.1 Scenario1, dependent fields
        when the underlying fields are dependent,
        even if they are all thread-safe respectively,
        for the "dependency" can not be preserved,
        which will result in invalid state of the current object when multiple threads tries to modify it.
        Solution is to use a common lock of to make the operation on the dependent fields atomic
    4.3.2 Scenario2, compound actions
        Solution is also by introducing a common lock,
        which will make the compound actions atomic

5. @GuardedBy:
      documentation purposed annotation,
      documents which lock they should acquire to guarantee thread-safety

6. Client side locking
    explicitly lock object/methods on the client side (client means the user of the class)

Building Blocks
1. Using iterators does not obviate the need to lock the collection during iteration
   if other threads can concurrently modify it.

2. ConcurrentHashMap,
    It is a replacement for synchronized hash-based map implementations,

3. CopyOnWriteArrayList,
    It is a replacement for synchronized List implementations for cases where traversal is the dominant operation.
    The copy-on-write collections derive their thread safety from the fact that as long as an effectively immutable object is properly published,
    no further synchronization is required when accessing it.
    They implement mutability by creating and republishing a new copy of the collection every time it is modified.
    Iterators for the copy-on-write collections retain a reference to the backing array that was current at the start of iteration,
    and since this will never change,
    they need to synchronize only briefly to ensure visibility of the array contents.
    Iterators return the elements exactly as they were at the creation time,
    regardless of subsequent modifications

4. CurrentMap interface,
    adds support for common compound actions
    such as put-if-absent, replace, and conditional remove

5. Queue and BlockingQueue,
  5.1 non-blocking queues (queue operations do not block), if queue is empty, the retrieval operation returns null
    ConcurrentLinkedQueue, traditional FIFO queue,
    PriorityQueue, a (non concurrent) priority ordered queue.
  5.2 blocking queues, blocking insertion and retrieval. If queue is empty, retrieval is blocked, If queue is full, insertion is blocked
    BlockingQueue

6. ConcurrentSkipListMap, ConcurrentSkipListSet
    are concurrent replacement for a synchronized SortedMap/SortedSet (such as TreeMap or TreeSet wrapped with synchronizedMap)

7. Iterators returned by ConcurrentHashMap are weakly consistent instead of fail-fast
   Weakly consistent iterator:
    can tolerate concurrent modification,
    traverses elements as they existed when the iterator was constructed,
    and may (but is not guaranteed to) reflect modifications to the collection after the construction of the iterator

8. Object pools exploit serial thread confinement,
   "lending" an object to a requesting thread.
   As long as the pool contains sufficient internal synchronization to publish the pooled object safely,
   and as long as the client do not themselves publish the pooled object or use it after returning it to the pool,
   ownership can be transferred safely from thread to thread

9. Work Stealing pattern
  9.1 Deque
      work stealing pattern can be implemented by using Deque (pronounced "deck"), which is double-ended queue
      it allows efficient insertion and removal from both the head and the tail.
      e.g. ArrayDeque, LinkedBlockingDeque
  9.2 compared with producer-consumer pattern
    9.2.1 work stealing pattern,
            every consumer has its own deque,
            if a consumer exhausts the work in its own deque,
            it can steal work from the tail of someone else's deque
    9.2.2 producer-consumer pattern has one shared work queue for all consumers
  9.3 Work stealing is well suited to problems in which consumers are also producers,
      when performing a unit of work is likely to result in the identification of more work

10. Synchronizer,
      any object that coordinates the control flow of threads based on its state.
      typical synchronizers are: BlockingQueue, semaphores, barriers, and latches.
      All synchronizers share certain structural properties:
        they **encapsulate state** that determines whether threads arriving at the synchronizer should be allowed to pass or forced to wait.
        provide **methods to manipulate that state**
        provide **methods to wait** efficiently for the synchronizer to enter the desired state.

  10.1 Latches
        A latch is a synchronizer that can delay the progress of thread until it reaches its terminal state.
        once the latch reaches the terminal state, it cannot change state again.
        latches can be used to ensure that certain activities do not proceed until one-time activities complete
  10.2 FutureTask (acts like a latch)
        FutureTask is used by the Executor framework,
        to represent asynchronous tasks,
        and the task may or may not have completed
        FutureTask implements Runnable interface
        and it tasks a Callable argument which represents the callback function
  10.3 Semaphores
        Counting semaphores are used to control **the number of activities**
        that can access a certain resource or perform a given action **at the same time**.
        counting semaphores can be used to implement resource pools or to impose a bound on a collection
  10.4 Barriers
        Similar to latches in that they block a group of threads until some event has occured.
        The key difference is that with a barrier,
        all the threads must come together at a barrier point **at the same time** in order to proceed
        Latches wait for events, barriers wait for other threads
          CyclicBarriers, allows a fixed number of parties to rendezvous repeatedly at a barrier point

Task Execution
1. The Executor Framework
  1.1 The primary abstraction for task execution in the Java class libraries is not Thread, but Executor
        public interface Executor{
          void execute(Runnable command);
        }

      Executor is based on the producer-consumer pattern,
      where activities that submit tasks are producers (producing units of work to be done)
      and threads that execute tasks are the consumers (consuming those units of work).
  1.2 Whenever you see code of the form:
        new Thread(runnable).start()
      and you think you might at some point want a more flexible execution policy,
      seriously consider replacing it with the use of an Executor
  1.3 Thread Pools
        Worker threads have a simple life:
        request the next task from the work queue, execute it, and go back to waiting for another task
        **Executors.newFixedThreadPool(NUMBER)  , is an example of creating thread pool (ExecutorService) by using Executor's factory methods
  1.4 Submitting task with Executor#execute() adds the task to work queue,
      and the worker threads repeatedly dequeue tasks from the work queue and execute them
  1.5 ExecutorService, adds lifecycle management methods to executors
      public interface ExecutorService extends Executor{
        void shutdown()
        boolean isShutdown();
        boolean isTerminated();
        boolean awaitTermination(long timeout, TimeUnit unit)
        ...
      }
  1.6 ExecutorService lifecycle states:
    1.6.1 running
    1.6.2 shutting down
    1.6.3 terminated

  1.7 Graceful shutting down:
        no new tasks are accepted
        but previously submitted tasks are allowed to complete
        including those that have not yet begun execution

  1.8 Abrupt shutdown:
        cancel outstanding tasks
        and does not start any tasks that are queued but not begun

  1.9 Delayed and Periodic Tasks
        Can be handled by ScheduledThreadPoolExecutor,
        which is a replacement of Timer,
        who manages execution on a "relative basis" in stead of Timer's absolute strategy,
        it compensates the dependency to the system clock, which is error prone
  1.10 Thread leakage:
        Tasks that are already scheduled but not yet executed are never run.
        (which can happen if an Exception is thrown in task scheduled by Timer)

2. Finding Exploitable Parallelism
  Trying to increase concurrency by parallelizing heterogeneous activities can be a lot of work,
  and there is a limit to how much additional concurrency you can get out of it.

Cancellation and ShutDown
1. There is no safe way to preemptively stop a thread in Java,
   and therefore no safe way to preemptively stop a task.
   There are only cooperative mechanisms,
   by which the task and the code requesting cancellation follow an agreed-upon protocol.
   the agreed-upon protocol is "setting a cancellation requested flag"

2. Thread#join(),
    means wait for the calling thread to die

3. Daemon Threads
  3.1 Sometimes you want to create a thread that perform some helper function
      but you don't want the existence of this thread to prevent the JVM from shutting down.
      You can use daemon threads
  3.2 Threads are divided into two types:
        When JVM starts,
        all threads it creates are daemon threads, e.g. garbage collector and other housekeeping threads
        except the main thread.
        Normal threads and daemon threads differ only when they exit
    3.2.1 Normal threads
    3.2.2 Daemon threads,
            when JVM halts,
            any remaining daemon threads are abandoned,
            daemon should be used sparingly, and should not perform anyI/O

4. using FutureTask and Executor framework simplifies building cancellable tasks and services

Applying Thread Pools
1. Thread pools work best when tasks are homogeneous and independent
  1.1 Mixing long-running and short-running tasks risks "clogging" the pool unless it is very large
  1.2 submitting tasks that dependent on other tasks risks deadlock unless the pool is unbounded.

2. Thread starvation deadlock
    Tasks in a queue wait for results from other tasks to proceed,
    but such results never come.

3. Long-running tasks
    If the wait times out, you can mark the task as failed and abort it or requeue it for execution later

4. Sizing the thread pool
  4.1 For compute-intensive tasks:
        an Ncpu processor system usually achieves optimum utilization with a thread pool of (Ncpu+1) threads
        Note: the extra runnable thread prevents CPU from going unused when page fault or pause for some other reason happens
  4.2 For tasks that also include I/O or other locking operations:
        you want a larger pool, since not all threads will be scheduled all the times.
  4.3 Calculating thread pool size:
        Ncpu = number of CPUs, int N_CPUS = Runtime.getRuntime().availableProcessors();
        Ucpu = target CPU utilization, 0<=Ucpu<=1
        W/C = ratio of wait time to compute time
        The optimal pool size for keeping the processor at the desired utilization is
          Nthreads = Ncpu * Ucpu * (1+W/C)
  4.4 Other than CPU, other resources can contribute to sizing constraints are memory, file handles, socket handles, and database connections.
      To calculate pool size constraints for these type of resources:
        add up how much of that resource each task requires,
        and divide that into the total quantity available.
        The result will be an upper bound on the pool size

5. Managing Queued Tasks
  5.1 With fixed sized thread pools,
      If the arrival rate for new requests exceed the rate at which they can be handled,
      requests will still queue up,
      they wait in a queue of Runnables managed by the Executor instead of queueing up as threads contending for the CPU.
  5.2 ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution.
      There are three basic approaches: unbounded queue, bounded queue, synchronous handoff.
      Note: synchronous handoff means,
            for very large or unbounded pools,
            you can bypass queueing entirely and instead hand off tasks directly from producers to worker threads.
            SynchronousQueue is not really a queue,
            but a mechanism for managing handoffs between threads.
            SynchronousQueue is a practical choice only if the pool is unbounded or if rejecting excess task is acceptable

6. Bounding either the **thread pool** or the **work queue** is suitable only when tasks are independent.
   With tasks that depend on other tasks,
   bounded thread pools or queues can cause thread starvation deadlock.
   instead, use an unbounded pool configuration like newCachedThreadPool.

7. Saturation policies (are preformed when queue for storing coming threads is full)

8. Hook:
    the notion of hook is "methods" to be overwrite on a component class,
    which will be further called by its manipulator (e.g. pool, container, etc.)

9. Loops whose bodies contain nontrivial computation or perform potentially blocking I/O are frequently good candidates for parallelization,
   as long as the iterations are independent.

Avoid liveness Hazards
1. A program will be free of locking-ordering deadlocks if all threads acquire the locks they need in a fixed global order.
   solution can be inducing a Global Lock Ordering,
   which can be implemented by using **System.identityHashCode()**,
   1.1 if two locks' systemHashCode are not equal, always acquire the smaller/larger one
   1.2 if two locks' systemHashCode are equal, means there is a hashCode collision, which can be solved by introducing another "tieLock" at the beginning

2. Open call:
    calling a method with no locks held
    using open calls to avoid deadlock is analogous to using encapsulation to provide thread safety

3. Restructuring a synchronized block to allow open calls can sometimes have undesirable consequences,
   since it takes an operation that was atomic and makes it not atomic
   Note: in many cases, the loss of atomicity is perfectly acceptable.
   In cases, the loss of atomicity is a problem,
   to reachieve atomicity, one technique is to structure a concurrent object so that only one thread can execute the code path following the open call
   Thus, rather than using locking to keep the order threads out of a critical section of code,
   this technique relies on constructing protocols so that other threads don't try to get in.

4. Timed Lock Attempts
  4.1 Intrinsic locks:
        wait forever if they cannot acquire the lock
  4.2 Explicit locks:
        let you specify a timeout, by using a timeout that is much longer than you expect acquiring the lock to take,
        you can regain control when something unexpected happens.

5. Deadlock Analysis with Thread Dumps
  5.1 Thread Dump is a JVM technique to analysis Deadlock
  5.2 A thread dump includes a stack trace for each running thread.
  5.3 Thread dump also include locking information, such as
        which locks are held by each thread
        in which stack frame they were acquired,
        which lock a blocked thread is waiting to acquire
  5.4 Trigger a thread dump:
    5.4.1 option1, send JVM process a SIGQUIT signal (kill -3)
    5.4.2 press ctrl-\ key
    5.4.3 request from IDE
  5.5 Thread dump tools
    5.5.1 OS native command kill -3 <PID>,
          Note: PID can be obtained from "ps -ef| grep java"
    5.5.2 JDK jstack utility,
          jstack <PID>
    5.5.3 Java Visual VM
    5.5.4 Oracle WebLogic Admin Console
  5.6 Thread dump analysis with Tomcat
        issue "kill -3 <TOMCAT_PID>", the dump will appear in TOMCAT/logs/catalina.out
        issue "jstack <TOMCAT_PID>", the dump will appear in the current console
  5.7 You can generate thread dump several times,
      by comparing the dump sampling results,
      you can better analysis the behavior
  5.8 Thread dump analysis
    5.8.1 Java head utilization is very useful for correlation with thread problem patterns such as internal contention
    5.8.2 differentiate healthy and non-healthy threads
      5.8.2.1 healthy: a thread waiting for a task
      5.8.2.2 non-healthy: a thread is executing a task for too long or STUCK
    5.8.3 Primary thread dump analysis objective: identify one or many **problem patterns**
    5.8.4 Always analysis thread stack trace from bottom up
    5.8.5 thread lock contention: identify the culprit thread holding the FLAT LOCK
    5.8.6 thread dump analysis exposing abnormal slowdown conditions,
          such as "logging", "class loading", "XML parsing", etc.
          is often the symptom of excessive JVM garbage collection,
          and/or Java Heap Depletion
    5.8.7 When potential problems are identified by analyzing thread stack trace,
          go to the "Heap Section" to see the root cause of the dead lock

6. Other liveness hazard problems
  6.1 Starvation:
        occurs when a thread is perpetually access to resources it needs in order to make progress
  6.2 Poor Responsiveness:
        like GUI frozen caused by heavy back ground computation in one thread
  6.3 Live Lock
        Live lock is a form of liveness failure in which a thread, while not blocked,
        still cannot progress because it keeps retrying an operation that will always fail
        (poison message problem)
        This form of livelock often caused from over eager error-recovery code that mistakenly treats an unrecoverable error as a recoverable one.
        Live lock can also occur when multiple cooperating threads change their states in response to the other in such a way that no thread can ever make progress,
        solution is to introduce randomness

7. Avoid the temptation to use thread priority to resolve dead lock,
   since they increase platform dependence and can cause liveness problems
   most concurrent applications can use the default priority for all threads

Performance and Scalability
