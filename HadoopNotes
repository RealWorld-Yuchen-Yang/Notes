I. Hadoop Fundamentals
Meet Hadoop
1. HDFS: Hadoop Distributed Filesystem
2. MapReduce is a batch query processor
3. Hadoop works well with semi-structured and unstructured data
    Semi-structured data: though there may be a schema, it is often ignored. like spreadsheet (whose structure is a grid of cells)
    Unstructured data: does not have any particular internal structure. like plain text, image
4. Data locality: Hadoop tries to co-locate data with compute nodes.
                  so data access is fast because it is local
5. MapReduce is designed to run jobs that last minutes or hours on trusted, dedicated hardware running in a single data center
   with very high aggregate bandwidth
6. The map function is a good place to drop bad records.
7. Data Flow
  7.1 MapReduce job:
        unit of work that the client wants to be performed.
        consists of:
          7.1.1 input data
          7.1.2 MapReduce program
          7.1.3 configuration information
  7.2 Hadoop runs the job by dividing it into tasks:
    7.2.1 map tasks
    7.2.2 reduce tasks
  7.3 Tasks are scheduled using YARN (Yet Another Resource Negotiator)
      tasks are run on nodes in clusters
      if a task fails, it will be automatically rescheduled to run on a different node.
  7.4 splits:
        Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits.
        Hadoop creates one map task for each split,
        which runs the user-defined map function for each record in the split
        A good split size tends to be the size of an HDFS block, which is 128MB by default
  7.5 Data locality optimization:
        Hadoop tries its best to run map tasks on the node where data originates
        (same node) > (same rack) > (different rack)
  7.6 Map tasks write their output to the local disk, for its output is intermediate result
  7.7 Reduce task to not have data locality -- input to a single reduce task is normally the output from **all** mappers
      for each HDFS block of the reduce output,
      the first replica is stored on the local node,
      with other replicas being stored on off-rack nodes for reliability
  7.8 When there are multiple reducers,
      the map tasks partition their output,
      each creating one partition for each reduce task.
      There can be many keys (and their associated values) in each partition,
      but the records for any given key are all in a single partition


The Hadoop Distributed System
1. hdfs fsck / -files -blocks,
    lists the blocks that make up each file in the filesystem.

2. Namenodes and Datanodes
    2.1 Namenodes, manage the filesystem namespace
    2.2 Datanodes, workhorses of the filesystem

3. failover controller, manages transition from the active namenode to the standby

4. fencing
      during failover management,
      make sure that the previously active namenode is prevented from doing any damage and causing corruption

5. Installing and Configure Hadoop
  # Installation (Mac)
  * **brew install hadoop**, installs hadoop
  * **hadoop version**, verifies installation
  # Configuration
  * brew installed configure file location: **/usr/local/Cellar/hadoop/X.X.X/libexec/etc/hadoop**
      * **core-site.xml**, common properties
      * **hdfs-site.xml**
      * **mapred-site.xml**
      * **yarn-site.xml**

    <?xml version="1.0"?>
    <!-- core-site.xml -->
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://localhost/</value>
        </property>
    </configuration>

    <?xml version="1.0"?>
    <!-- hdfs-site.xml -->
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>1</value>
        </property>
    </configuration>

    <?xml version="1.0"?>
    <!-- mapred-site.xml -->
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>

    <?xml version="1.0"?>
    <!-- yarn-site.xml -->
    <configuration>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>localhost</value>
        </property>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    </configuration>

  * customizing config files
      * declare config file location environment variable **HADOOP_CONF_DIR**, then export, or declare and export in .bash_profile
      * declare the above four config files

  # Format the HDFS before running Hadoop
  **hfs namenode -format**

  # sbin directory: /usr/local/Cellar/hadoop/2.7.2/sbin, bin directory is already exported once brew-installed
  (Note: running the following scripts are before logging into to the HDFS, also include sbin in the environment path is convenient)

  * start/stop services individually
      * start-dfs.sh
      * start-yarn.sh
      * mr-jobhistory-daemon.sh, mr-jobhistory-daemon.sh [start|stop] historyserver
      * stop-dfs.sh
      * stop-yarn.sh

  * start/stop services in groups by declaring alias in .bash_profile
      * start hadoop services, **alias hstart="start-dfs.sh;start-yarn.sh;mr-jobhistory-daemon.sh start history server"**
      * stop hadoop services, **alias hstop="stop-yarn.sh;stop-dfs.sh;mr-jobhistory-daemon.sh stop history server"**

  # Check running java jobs by using command: **jps**

  # Pseudo Distributed Mode
  * default Web UI
      * namenode: http://localhost:50070/
      * resource manager: http://localhost:8088/
      * history server: http://localhost:19888/

  Note: either Web UI or log directory (in Hadoop installation directory **/usr/local/Cellar/hadoop/2.7.2/libexec/logs**) can be used to verify running status

  # Useful External Links
  * https://dtflaneur.wordpress.com/2015/10/02/installing-hadoop-on-mac-osx-el-capitan/
  * See also HadoopNotes for config details, especially xml https://github.com/RealWorld-Yuchen-Yang/Notes/blob/master/HadoopNotes

6. CLI
  6.1 Hadoop can be run in three modes
    6.1.1 standalone, no daemon, everything runs in a single JVM, suitable for development
    6.1.2 pseudodistributed, daemons run on the local machine, simulate small scale cluster
    6.1.3 fully distributed, daemons run on a cluster of machines

7. hadoop fs -ls DIRECTORY, lists detailed information of the DIRECTORY
    the second column of the list result is the **replication factor**
    entry in this column is empty for directories
    directories are treated as metadata and stored by the namenode, not datanode

8. Files in HDFS can not have "execution" permission,
   because you can not execute a file on HDFS
   directories can have "execution" permission, which means it can access it children

9. There is a concept of a superuser,
   which is the identity of the **namenode process**
   Permissions checks are not performed for the superuser

10. HDFS can only write to the end of a file,
    so appending to a file works,
    but random modifications of a file do not.

11. It is possible to mount HDFS on a local client's filesystem using Hadoop's NFSv3 gateway.
    You can then use Unix utilities (such as ls and cat) to interact with the filesystem.

12. Network topology and Hadoop
  What does it mean for two nodes in a local network to be "close" to each other?
  The idea is to use the bandwidth between two nodes as a measure of distance
  Hadoop does not measure the bandwidth between two nodes,
  instead, it takes a single approach,
  it view the network as a tree and the distance between two nodes is the sum of their distance to their closet common ancestor
  The idea is that bandwidth available for each of the following scenarios becomes progressively less:
    12.1 processes on the same node
    12.2 different nodes on the same rack
    12.3 nodes on different racks, but in the same data center
    12.4 nodes in different data ceneters

YARN (Yet Another Resource Negotiator)
1. YARN provides APIs for requesting and working with cluster resources,
   but these APIs are not typically used directly by user code.
   Users write higher-level APIs (MapReduce, Spark, Tez, etc.) provided by distributed computing frameworks,
   which themselves are built on YARN
   Also Processing frameworks like Pig, Hive, Crunch, etc. are built on MapReduce, Spark, Tez, etc.

2. Two YARN long-running daemons:
  2.1 resource manager,
        one per cluster
        manage the use of resources across the cluster
  2.2 node manager,
        all nodes in the cluster
        launch and monitor containers

Hadoop I/O
1. which compression format should I use? principles are listed as follows
  1.1 use a container file format such as sequence files, Avro datafiles, ORCFiles,
      Parquet files, all of which support both compression and splitting.
      A fast compressor such as LZO, LZ4, or Snappy is generally a good choice.
  1.2 Use a compression format that supports splitting, such as bzip2, or one can be indexed to support splitting, such as LZO
  1.3 Split the file into chunks in the application, and compress each chunk separately using any supported compression format
      In this case, you should choose the chunk size so that the compressed chunks are approximately the size of an HDFS block
  1.4 Store the files uncompressed

2. Writable Interface and Writable Wrappers
    2.1 Writable interface defines two methods:
          * write(DataOutput out) throws IOException
          * readFields(DataInput in) throws IOException
          Note: DataOutput&DataInput wraps ByteArrayInput/OutputStream
    2.2 wrapper classes implements WritableComparable interface,
        which is a subinterface of the Writable and java.lang.Comparable interfaces
        **public interface WritableComparable<T> extends Writable, Comparable<T> {}**

3. MapFile
  A MapFile is a sorted SequenceFile with an index to permit lookups by key.
  The index is itself a SequenceFile that contains a fraction of keys in the map.
  (every 128th key, by default)
  The idea is that the index can be loaded into memory to provide fast lookups from the main data file,
  which is another SequenceFile containing all the map entries in sorted key order.

4. Sequence files, map files, and Avro datafiles are all row-oriented file formats,
   4.1 row-oriented file, the values for each row are stored contiguously in the file.
                          row-oriented formats are appropriate when a large number of columns of a single row are needed for processing at the same time
   4.2 column-oriented file, rows in a file are broken up into row splits,
                             then each spit is stored in column-oriented fashion,
                             where values for each row in the first column are stored first,
                             followed by the values for each row in the second column,
                             and so on.
                             A column-oriented layout permits columns that are not accessed in a query to be skipped
                             column-oriented formats work well when queries access only a small number of columns in the table.
                             e.g. ORCFile (Optimized Record Columnar File), Parquet, Trevni

Developing a MapReduce Application
1. Writing a program in MapReduce follows a certain pattern.
  1.0 set up and configure development environment
  1.1 start by writing your map and reduce functions
  1.2 write unit tests to make sure these functions all work
  1.3 write a driver program to run the program (Like YARN),
      run the program with small subset of data to check it is working
  1.4 with this information, you can expand your unit tests to cover this case
      and improve your mapper or reducer as appropriate to handle such input correctly.
  1.5 improve the program's efficiency
  1.6 task profiling

2. Shuffle and Sort
    MapReduce makes the guarantee that the input to every reducer is sorted by key.
    2.1 Shuffle: The process by which the system performs the sort,
                 and transfers the map outputs to the reducers as inputs

3. MapReduce configuration tuning
  3.1 The general principle is to give the shuffle as much memory as possible.
      However, there is a trade-off,
      in that you need to make sure that your map and reduce functions gets enough memory to operate.
      This is why it is best to write your map and reduce functions to use as little memory as possible
  3.2 On the map side,
      the best performance can be obtained by avoiding multiple spills to disk
      one is optimal
  3.3 On the reduce side,
      the best performance is obtained when the intermediate data can reside entirely in memory

4. Speculative execution of tasks
    Hadoop doesn't try to diagnose and fix slow-running tasks
    instead, it tries to detect when a task is running slower than expected
    and launches another equivalent task as a backup
