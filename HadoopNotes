I. Hadoop Fundamentals
Meet Hadoop
1. HDFS: Hadoop Distributed Filesystem
2. MapReduce is a batch query processor
3. Hadoop works well with semi-structured and unstructured data
    Semi-structured data: though there may be a schema, it is often ignored. like spreadsheet (whose structure is a grid of cells)
    Unstructured data: does not have any particular internal structure. like plain text, image
4. Data locality: Hadoop tries to co-locate data with compute nodes.
                  so data access is fast because it is local
5. MapReduce is designed to run jobs that last minutes or hours on trusted, dedicated hardware running in a single data center
   with very high aggregate bandwidth
6. The map function is a good place to drop bad records.
7. Data Flow
  7.1 MapReduce job:
        unit of work that the client wants to be performed.
        consists of:
          7.1.1 input data
          7.1.2 MapReduce program
          7.1.3 configuration information
  7.2 Hadoop runs the job by dividing it into tasks:
    7.2.1 map tasks
    7.2.2 reduce tasks
  7.3 Tasks are scheduled using YARN (Yet Another Resource Negotiator)
      tasks are run on nodes in clusters
      if a task fails, it will be automatically rescheduled to run on a different node.
  7.4 splits:
        Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits.
        Hadoop creates one map task for each split,
        which runs the user-defined map function for each record in the split
        A good split size tends to be the size of an HDFS block, which is 128MB by default
  7.5 Data locality optimization:
        Hadoop tries its best to run map tasks on the node where data originates
        (same node) > (same rack) > (different rack)
  7.6 Map tasks write their output to the local disk, for its output is intermediate result
  7.7 Reduce task to not have data locality -- input to a single reduce task is normally the output from **all** mappers
      for each HDFS block of the reduce output,
      the first replica is stored on the local node,
      with other replicas being stored on off-rack nodes for reliability
  7.8 When there are multiple reducers,
      the map tasks partition their output,
      each creating one partition for each reduce task.
      There can be many keys (and their associated values) in each partition,
      but the records for any given key are all in a single partition


The Hadoop Distributed System
1. hdfs fsck / -files -blocks,
    lists the blocks that make up each file in the filesystem.

2. Namenodes and Datanodes
    2.1 Namenodes, manage the filesystem namespace
    2.2 Datanodes, workhorses of the filesystem

3. failover controller, manages transition from the active namenode to the standby

4. fencing
      during failover management,
      make sure that the previously active namenode is prevented from doing any damage and causing corruption

5. Installing and Configure Hadoop
  # Installation (Mac)
  * **brew install hadoop**, installs hadoop
  * **hadoop version**, verifies installation
  # Configuration
  * brew installed configure file location: **/usr/local/Cellar/hadoop/X.X.X/libexec/etc/hadoop**
      * **core-site.xml**, common properties
      * **hdfs-site.xml**
      * **mapred-site.xml**
      * **yarn-site.xml**

  * customizing config files
      * declare config file location environment variable **HADOOP_CONF_DIR**, then export, or declare and export in .bash_profile
      * declare the above four config files

  # Format the HDFS before running Hadoop
  **hfs namenode -format**

  # sbin directory: /usr/local/Cellar/hadoop/2.7.2/sbin, bin directory is already exported once brew-installed
  (Note: running the following scripts are before logging into to the HDFS, also include sbin in the environment path is convenient)

  * start/stop services individually
      * start-dfs.sh
      * start-yarn.sh
      * mr-jobhistory-daemon.sh, mr-jobhistory-daemon.sh [start|stop] historyserver
      * stop-dfs.sh
      * stop-yarn.sh

  * start/stop services in groups by declaring alias in .bash_profile
      * start hadoop services, **alias hstart="start-dfs.sh;start-yarn.sh;mr-jobhistory-daemon.sh start history server"**
      * stop hadoop services, **alias hstop="stop-yarn.sh;stop-dfs.sh;mr-jobhistory-daemon.sh stop history server"**

  # Check running java jobs by using command: **jps**

  # Pseudo Distributed Mode
  * default Web UI
      * namenode: http://localhost:50070/
      * resource manager: http://localhost:8088/
      * history server: http://localhost:19888/

  Note: either Web UI or log directory (in Hadoop installation directory **/usr/local/Cellar/hadoop/2.7.2/libexec/logs**) can be used to verify running status

  # Useful External Links
  * https://dtflaneur.wordpress.com/2015/10/02/installing-hadoop-on-mac-osx-el-capitan/
  * See also HadoopNotes for config details, especially xml https://github.com/RealWorld-Yuchen-Yang/Notes/blob/master/HadoopNotes

6. CLI
  6.1 Hadoop can be run in three modes
    6.1.1 standalone, no daemon, everything runs in a single JVM, suitable for development
    6.1.2 pseudodistributed, daemons run on the local machine, simulate small scale cluster
    6.1.3 fully distributed, daemons run on a cluster of machines

7. 
